%! Author = Kevin Lin
%! Date = 2/9/2026

% Preamble
\documentclass[11pt,a4paper,margin=1in]{article}

% Packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{float}

\title{HW 2}
\author{Kevin Lin}
\date{2/9/2026}

% Document
\begin{document}
\maketitle

\section{}
\begin{enumerate}[(a)]
    \item The gradient of hinge loss with respect to $w$:
    \begin{align*}
        \nabla_w \ell(w^Tx, y) &= \nabla_w \max\{0, 1 - y w^T x\} \\
        &= \begin{cases}
            0 & \text{if } y w^T x \geq 1 \\
            -y x & \text{if } y w^T x < 1
        \end{cases}
    \end{align*}
    \item The gradient of Perceptron loss w.r.t $w$:
    \begin{align*}
        \nabla_w \ell(w^Tx, y) &= \nabla_w \max\{0, - y w^T x\} \\
        &= \begin{cases}
            0 & \text{if } y w^T x \geq 0 \\
            -y x & \text{if } y w^T x < 0
        \end{cases}
    \end{align*}
    \item For hinge loss, $w$ is updated only when the margin condition $y w^T x < 1$ 
    is violated, meaning the prediction is not only incorrect but also not 
    confident enough. However, for Perceptron loss, the update occurs whenever 
    the prediction is incorrect (when $y w^T x < 0$). This means that hinge loss 
    encourages a larger margin between classes while Perceptron loss focuses 
    solely on correct classification.
\end{enumerate}

\section{}
\begin{enumerate}[(a)]
    \item ROC plot (see Jupyter notebook for code): \\
    \includegraphics[width=\textwidth]{2a.png}
    \item Classifier 2 has the highest accuracy of 0.8, while Classifier 6 has 
    the lowest accuracy of 0.36. See Jupyter notebook for code.
    \item Classifier 5 has the highest precision of 1, while Classifier 1 has
    the lowest precision of 0.64. Classifier 6 has undefined precision as it has
    no true or false positives. See Jupyter notebook for code.
    \item Classifier 1 F1 score: 0.78 \\
    Classifier 2 F1 score: 0.86 \\
    Classifier 3 F1 score: 0.81 \\
    Classifier 4 F1 score: 0.71 \\
    Classifier 5 F1 score: 0.4 \\
    Classifier 6 F1 score: Undefined for the same reasons as precision.\\
    See Jupyter notebook for code.
\end{enumerate}

\section{}
See Jupyter notebook for code.
\begin{enumerate}[(a)]
    \item \begin{verbatim}
    Margins for each data point:
    Point (2, 2, 3) with label 1: 0.74
    Point (3, 3, 2) with label 1: 0.93
    Point (1, 2, 3) with label 1: 0.19
    Point (1, 4, 1) with label 1: -0.56
    Point (4, 4, 4) with label 1: 3.34
    Point (2, 2, 2) with label 1: 0.00
    Point (3, 3, 1) with label -1: -0.19
    Point (1, 1, 1) with label -1: 1.67
    Point (3, 2, 2) with label -1: -0.56
    Point (0, 4, 2) with label -1: 0.37
    Point (4, 0, 0) with label -1: 1.11
    Point (0, 0, 3) with label -1: 1.11
    \end{verbatim}
    \item \begin{verbatim}
    0-1 Loss for each data point:
    Point (2, 2, 3) with label 1: 0
    Point (3, 3, 2) with label 1: 0
    Point (1, 2, 3) with label 1: 0
    Point (1, 4, 1) with label 1: 1
    Point (4, 4, 4) with label 1: 0
    Point (2, 2, 2) with label 1: 0
    Point (3, 3, 1) with label -1: 1
    Point (1, 1, 1) with label -1: 0
    Point (3, 2, 2) with label -1: 1
    Point (0, 4, 2) with label -1: 0
    Point (4, 0, 0) with label -1: 0
    Point (0, 0, 3) with label -1: 0
    \end{verbatim}
    \item \begin{verbatim}
    Hinge Loss for each data point:
    Point (2, 2, 3) with label 1: 0
    Point (3, 3, 2) with label 1: 0
    Point (1, 2, 3) with label 1: 0
    Point (1, 4, 1) with label 1: 4
    Point (4, 4, 4) with label 1: 0
    Point (2, 2, 2) with label 1: 1
    Point (3, 3, 1) with label -1: 2
    Point (1, 1, 1) with label -1: 0
    Point (3, 2, 2) with label -1: 4
    Point (0, 4, 2) with label -1: 0
    Point (4, 0, 0) with label -1: 0
    Point (0, 0, 3) with label -1: 0
    \end{verbatim}
    \item \begin{verbatim}
    Squared Loss for each data point:
    Point (2, 2, 3) with label 1: 9
    Point (3, 3, 2) with label 1: 16
    Point (1, 2, 3) with label 1: 0
    Point (1, 4, 1) with label 1: 16
    Point (4, 4, 4) with label 1: 289
    Point (2, 2, 2) with label 1: 1
    Point (3, 3, 1) with label -1: 4
    Point (1, 1, 1) with label -1: 64
    Point (3, 2, 2) with label -1: 16
    Point (0, 4, 2) with label -1: 1
    Point (4, 0, 0) with label -1: 25
    Point (0, 0, 3) with label -1: 25
    \end{verbatim}
\end{enumerate}

\section{}
\begin{enumerate}[(a)]
    \item We can derive the optimal $\textbf{w}^*$ that minimizes $E_2{\textbf{w}}$ as follows:
    \begin{align*}
        E_2(\textbf{w}) &= \frac{1}{N} \|\textbf{Xw} - \textbf{y}\|^2_2 + \lambda \|\textbf{w}\|^2_2 \\
        &= \frac{1}{N} (\textbf{Xw} - \text{y})^T (\textbf{Xw} - \textbf{y}) + \lambda \textbf{w}^T \textbf{w} \\
        &= \frac{1}{N} (\textbf{w}^T \textbf{X}^T \textbf{Xw} - 2 \textbf{y}^T \textbf{Xw} + \textbf{y}^T \textbf{y}) + \lambda \textbf{w}^T \textbf{w}
    \end{align*}
    Taking the gradient with respect to $\textbf{w}$ and setting it to zero:
    \begin{align*}
        \nabla_{\textbf{w}} E_2(\textbf{w}) &= \frac{2}{N} \textbf{X}^T \textbf{Xw} - \frac{2}{N} \textbf{X}^T \textbf{y} + 2 \lambda \textbf{w} = 0 \\
        &= (\textbf{X}^T \textbf{X} + N \lambda \textbf{I}) \textbf{w} = \textbf{X}^T \textbf{y} \\
        \textbf{w}^* &= (\textbf{X}^T \textbf{X} + N \lambda \textbf{I})^{-1} \textbf{X}^T \textbf{y}
    \end{align*}
    \item This new objective function overcomes the singularity issue by adding
    the term $\lambda \|\textbf{w}\|^2_2$, which effectively adds $\lambda \textbf{I}$ to the matrix 
    $\textbf{X}^T \textbf{X}$. This addition ensures that the matrix 
    $\textbf{X}^T \textbf{X} + N \lambda \textbf{I}$ is positive definite and invertible, even if 
    $\textbf{X}^T \textbf{X}$ is singular. The regularization term penalizes large weights,
    promoting stability and preventing overfitting.
\end{enumerate}

\section{}
See Jupyter notebook for code.
\begin{verbatim}
L2 Norm Differences of Least Squares Regression:
With Regularization:
GD vs SGD: 7.006176226969802
GD vs Closed Form: 7.021520447813614
SGD vs Closed Form: 0.08173535002944027
Without Regularization:
GD vs SGD: 5.282858727935199
GD vs Closed Form: 7.021521174825259
SGD vs Closed Form: 2.533883001083749
\end{verbatim}

\section{}
See Jupyter notebook for code (objective function values are also calculated in 
the same codeblock of Q5).
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{6gd.png}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{6sgd.png}
\end{figure}

\section{}
See Jupyter notebook for code.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{7.png}
\end{figure}

\section{}
We are given hat matrix $\textbf{H} = \textbf{X}(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T$, 
where $\textbf{X}$ is $N \times (d+1)$, and $\mathbf{X}^T\mathbf{X}$ is invertible.
\begin{enumerate}[(a)]
    \item We can show $\textbf{H}$ is symmetric:
    \begin{align*}
        \textbf{H}^T &= (\textbf{X}(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T)^T \\
        &= \textbf{X}(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T \\
        &= \textbf{H}
    \end{align*}
    \item We can show that $\textbf{H}^K = \textbf{H}$ for any integer $K \geq 1$ using induction: \\
    \begin{align*}
        HH &= \textbf{X}(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T \textbf{X}(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T \\
        &= \textbf{X}(\textbf{X}^T\textbf{X})^{-1} (\textbf{X}^T\textbf{X}) (\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T \\
        &= \textbf{X}(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T \\
        &= \textbf{H}
    \end{align*}
    Thus, by induction, $\textbf{H}^K = \textbf{H}$ for all integers $K \geq 1$.
    \item Given identity matrix $\textbf{I}$ of size $N$, then $(\textbf{I} - \textbf{H})^K = \textbf{I} - \textbf{H}$ for any integer $K \geq 1$:
    \begin{align*}
        (\textbf{I} - \textbf{H})^2 &= \textbf{I} - 2\textbf{H} + \textbf{H}^2 \\
        &= \textbf{I} - 2\textbf{H} + \textbf{H} \quad (\text{from part (b)}) \\
        &= \textbf{I} - \textbf{H}
    \end{align*}
    Thus, by induction, $(\textbf{I} - \textbf{H})^K = \textbf{I} - \textbf{H}$ for all integers $K \geq 1$.
    \item We can show that $\text{trace}(\textbf{H}) = d + 1$:
    \begin{align*}
        \text{trace}(\textbf{H}) &= \text{trace}(\textbf{X}(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T) \\
        &= \text{trace}((\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{X}) \quad (\text{by cyclic property of trace}) \\
        &= \text{trace}(\textbf{I}_{(d+1) \times (d+1)}) \\
        &= d + 1
    \end{align*}
\end{enumerate}

\includepdf[pages=-]{hw2_code.pdf}

\end{document}